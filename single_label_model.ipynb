{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import logictensornetworks as ltn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb749ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the AntiHypertensiveTherapy dataset from the Vsac website\n",
    "hpyertensive = pd.read_excel('../Haoming/AntiHypertensiveTherapy.xlsx', 'Expansion List')\n",
    "hypertensive = hpyertensive.loc[12:, 'Unnamed: 0']\n",
    "hypertensive = np.unique(hypertensive)\n",
    "for i in range(len(hypertensive)):\n",
    "    hypertensive[i] = 'rxn_' + hypertensive[i]\n",
    "    hypertensive.append('rxn_' + hypertensive[i] + 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c96ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the target result (GOLD_multiclass.csv) we want our model to generate\n",
    "\n",
    "target = pd.read_csv('../Haoming/target_result/GOLD_multiclass.csv', index_col=0)\n",
    "target.drop(index=225, inplace=True) # no.225 data is lost\n",
    "\n",
    "train_target = target[target['train'] == 1].drop(['train', 'test'], axis=1)\n",
    "test_target = target[target['test'] == 1].drop(['train', 'test'], axis=1)\n",
    "hypertension_target = train_target[\"Hypertension (int)\"]\n",
    "hypertension_test_target = test_target[\"Hypertension (int)\"]\n",
    "\n",
    "# absent = train_target[train_target == -1].index.tolist()\n",
    "# questionable = train_target[train_target == 0].index.tolist()\n",
    "# present = train_target[train_target == 1].index.tolist()\n",
    "# unmentioned = train_target[train_target == 3].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the feature matrix\n",
    "\n",
    "feature_matrix = pd.read_csv('../Haoming/additional/_feature_matrix_all_sections_.csv', index_col=0)\n",
    "feature_matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9210cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate the fm to train feature and test feature also intersect with the AntiHypertensiveTherapy dataset\n",
    "\n",
    "train_feature = feature_matrix.loc[train_target.index]\n",
    "test_feature = feature_matrix.loc[test_target.index]\n",
    "hypertension_feature = train_feature[train_feature.columns.intersection(hypertensive)]\n",
    "hypertension_test_feature = test_feature[test_feature.columns.intersection(hypertensive)]\n",
    "hypertensive = train_feature.columns.intersection(hypertensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b73f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6181e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for negation you need to abstract the count in the big fm dataset and \n",
    "#add new column to count the negation count of each standard medical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 * strategey.num_replicas_in_sync\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((hypertension_feature,hypertension_target)).batch(batch_size)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((hypertension_test_feature,hypertension_test_target)).batch(batch_size)\n",
    "ds_train = strategy.experimental_distribute_dataset(ds_train)\n",
    "ds_test = strategy.experimental_distribute_dataset(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(hypertensive)\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, n_classes, hidden_layer_sizes=(size,size,size/2)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.denses = [tf.keras.layers.Dense(s, activation=\"elu\") for s in hidden_layer_sizes]\n",
    "        self.dense_class = tf.keras.layers.Dense(n_classes)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for dense in self.denses:\n",
    "            x = dense(x)\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.dense_class(x)\n",
    "\n",
    "with strategy.scope():\n",
    "    logits_model = MLP(3)\n",
    "    p = ltn.Predicate(ltn.utils.LogitsToPredicateModel(logits_model,single_label=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_absent = ltn.Constant(0, trainable=False)\n",
    "class_present = ltn.Constant(1, trainable=False)\n",
    "class_questionable = ltn.Constant(2, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=6),semantics=\"exists\")\n",
    "\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def axioms(feautures, labels, training=False):\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    x_absent = ltn.Variable(\"x_absent\", features[labels==-1])\n",
    "    x_present = ltn.Variable(\"x_present\", features[labels==1])\n",
    "    x_questionable = ltn.Variable(\"x_questionable\", features[labels==0])\n",
    "    axioms = [\n",
    "        Forall(x_absent, p([x_absent, class_absent], training=training)),\n",
    "        Forall(x_present, p([x_present, class_present], training=training)),\n",
    "        Forall(x_questionable, p([x_questionable, class_questionable], training=training)),\n",
    "        Forall(x, Not(And(p([x, class_present]), p([x, class_absent])))),\n",
    "        Forall(x, Not(And(p([x, class_present]), p([x, class_questionable])))),\n",
    "        Forall(x, Not(And(p([x, class_questionable]), p([x, class_absent])))),\n",
    "    ]\n",
    "    \n",
    "\n",
    "    return formula_aggregator(axioms).tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f494696",
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, labels in ds_train:\n",
    "    print(\"Initial sat level %.5f\"%axioms(features,labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc40d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    metrics_dict = {\n",
    "        'train_axiom_sat':tf.keras.metrics.Mean(name='train_axiom_sat'),\n",
    "        'test_axiom_sat':tf.keras.metrics.Mean(name='test_axiom_sat')\n",
    "        'train_accuracy': tf.keras.metircs.SparseCategoricalAccuracy(name=\"train_accuracy\"),\n",
    "        'test_accuracy': tf.keras.metircs.SparseCategoricalAccuracy(name=\"test_accuracy\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc624a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        sat = axioms(features, labels, Training=True)\n",
    "        loss_value = 1. - sat\n",
    "    grads = tape.gradient(loss_value, p.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    sat = axioms(features, labels)\n",
    "    metrics_dict['train_axiom_sat'](sat)\n",
    "    predictions = logits_model(features)\n",
    "    metrics_dict['train_accuracy'](tf.one_hot(labels, 3), predictions)\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def test_step(features, labels):\n",
    "    sat = axioms(features, labels)\n",
    "    metrics_dict['test_axiom_sat'](sat)\n",
    "    predictions = logits_model(features)\n",
    "    metrics_dict['test_accuracy'](tf.one_hot(labels, 3), predictions)\n",
    "    \n",
    "@tf.function\n",
    "def distributed_train_step(features, labels):\n",
    "    strategy.run(train_step, args=(features, labels,))\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(features, labels):\n",
    "    strategy.run(test_step, args=(features, labels,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "commons.train(\n",
    "    EPOCHS,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    distributed_train_step,\n",
    "    distributed_test_step,\n",
    "    csv_path='',\n",
    "    track_metrics=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
