{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import logictensornetworks as ltn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e85acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('../Haoming/target_result/GOLD_multiclass.csv', index_col=0)\n",
    "target.drop(index=225, inplace=True) # no.225 data is lost\n",
    "\n",
    "train_target = target[target['train'] == 1].drop(['train', 'test'], axis=1)\n",
    "test_target = target[target['test'] == 1].drop(['train', 'test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = pd.read_csv('../Haoming/additional/_feature_matrix_all_sections_.csv', index_col=0)\n",
    "feature_matrix.fillna(0, inplace=True)\n",
    "\n",
    "train_feature = feature_matrix.loc[train_target.index]\n",
    "test_feature = feature_matrix.loc[test_target.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96766c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = target.columns.values[0:-2]\n",
    "class_name = []\n",
    "for i in range(32):\n",
    "    name  = entities[i]\n",
    "    class_name.append(name + '_absent')\n",
    "    class_name.append(name + '_questionable')\n",
    "    class_name.append(name + '_present')\n",
    "    if i%2 == 1:\n",
    "        class_name.append(name + '_unmentioned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce915271",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 * strategey.num_replicas_in_sync\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((train_feature,train_target)).batch(batch_size)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_feature,test_target)).batch(batch_size)\n",
    "ds_train = strategy.experimental_distribute_dataset(ds_train)\n",
    "ds_test = strategy.experimental_distribute_dataset(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE=len(feature_matrix.columns)\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, n_classes, hidden_layer_sizes=(SIZE, SIZE, SIZE/2)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.denses = [tf.keras.layers.Dense(s, activation=\"elu\") for s in hidden_layer_sizes]\n",
    "        self.dense_class = tf.keras.layers.Dense(n_classes)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for dense in self.denses:\n",
    "            x = dense(x)\n",
    "        return self.dense_class(x)\n",
    "\n",
    "with strategy.scope():\n",
    "    logits_model = MLP(len(class_name))\n",
    "    p = ltn.Predicate(ltn.utils.LogitsToPredicateModel(logits_model,single_label=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e075eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {}\n",
    "for i in range(len(class_name)):\n",
    "    classes.update({class_name[i]: ltn.Constant(i, trainable=False)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b556e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=2),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=6),semantics=\"exists\")\n",
    "\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def axioms(features, labels):\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    x_class = {}\n",
    "    for i in len(class_name):\n",
    "        r = i % 7\n",
    "        a = -99\n",
    "        if r == 0 or r == 3:\n",
    "            a = 1\n",
    "        elif r == 1 or r == 4:\n",
    "            a = -1\n",
    "        elif r == 2 or r == 5:\n",
    "            a = 0\n",
    "        else:\n",
    "            a = 3\n",
    "            \n",
    "        x_class[class_name[i]] = ltn.Variable(class_name[i], features[labels[entities[r]]==a])\n",
    "    \n",
    "    axioms = [\n",
    "        Forall(x_class[name], p([x_class[name], classes[name]])) for name in class_name\n",
    "    ]\n",
    "    for name1 in class_name:\n",
    "        for name2 in class_name:\n",
    "            if name1 != name2:\n",
    "                axioms.append(Forall(x, Not(And(p([x_class[name1], classes[name1]]), p([x_class[name2], classes[name2]])))))\n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2266a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, labels in ds_train:\n",
    "    print(\"Initial sat level %.5f\"%axioms(features,labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    metrics_dict = {\n",
    "        'train_axiom_sat':tf.keras.metrics.Mean(name='train_axiom_sat'),\n",
    "        'test_axiom_sat':tf.keras.metrics.Mean(name='test_axiom_sat')\n",
    "        'train_accuracy': tf.keras.metircs.SparseCategoricalAccuracy(name=\"train_accuracy\"),\n",
    "        'test_accuracy': tf.keras.metircs.SparseCategoricalAccuracy(name=\"test_accuracy\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d27773",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        sat = axioms(features, labels, Training=True)\n",
    "        loss_value = 1. - sat\n",
    "    grads = tape.gradient(loss_value, p.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    sat = axioms(features, labels)\n",
    "    metrics_dict['train_axiom_sat'](sat)\n",
    "    predictions = logits_model(features)\n",
    "    arr = []\n",
    "    for i in len(class_name):\n",
    "        r = i % 7\n",
    "        a = ''\n",
    "        if r == 0 or r == 3:\n",
    "            a = '1'\n",
    "        elif r == 1 or r == 4:\n",
    "            a = '-1'\n",
    "        elif r == 2 or r == 5:\n",
    "            a = '0'\n",
    "        else:\n",
    "            a = '3'\n",
    "        arr.append((class_name==a))\n",
    "    onehot = tf.stack(arr, aixs=-1)\n",
    "    metrics_dict['train_accuracy'](1-multilabel_hamming_loss(onehot, predictions, from_logits=True))\n",
    "    \n",
    "@tf.function\n",
    "def test_step(features, labels):\n",
    "    sat = axioms(features, labels)\n",
    "    metrics_dict['test_axiom_sat'](sat)\n",
    "    predictions = logits_model(features)\n",
    "    arr = []\n",
    "    for i in len(class_name):\n",
    "        r = i % 7\n",
    "        a = ''\n",
    "        if r == 0 or r == 3:\n",
    "            a = '1'\n",
    "        elif r == 1 or r == 4:\n",
    "            a = '-1'\n",
    "        elif r == 2 or r == 5:\n",
    "            a = '0'\n",
    "        else:\n",
    "            a = '3'\n",
    "        arr.append((class_name==a))\n",
    "    onehot = tf.stack(arr, aixs=-1)\n",
    "    metrics_dict['test_accuracy'](1-multilabel_hamming_loss(onehot, predictions, from_logits=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def distributed_train_step(features, labels):\n",
    "    strategy.run(train_step, args=(features, labels,))\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(features, labels):\n",
    "    strategy.run(test_step, args=(features, labels,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "commons.train(\n",
    "    EPOCHS,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    distributed_train_step,\n",
    "    distributed_test_step,\n",
    "    csv_path='',\n",
    "    track_metrics=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
