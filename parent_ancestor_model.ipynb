{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b224ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import logictensornetworks as ltn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('../Haoming/target_result/GOLD_multiclass.csv', index_col=0)\n",
    "target.drop(index=225, inplace=True) # no.225 data is lost\n",
    "\n",
    "train_target = target[target['train'] == 1].drop(['train', 'test'], axis=1)\n",
    "test_target = target[target['test'] == 1].drop(['train', 'test'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea882fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = pd.read_csv('../Haoming/additional/_feature_matrix_all_sections_.csv', index_col=0)\n",
    "feature_matrix.fillna(0, inplace=True)\n",
    "\n",
    "train_feature = feature_matrix.loc[train_target.index]\n",
    "test_feature = feature_matrix.loc[test_target.index]\n",
    "\n",
    "features = train_feature.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = target.columns.values[0:-2]\n",
    "class_name = []\n",
    "for i in range(32):\n",
    "    name  = entities[i]\n",
    "    class_name.append(name + '_absent')\n",
    "    class_name.append(name + '_questionable')\n",
    "    class_name.append(name + '_present')\n",
    "    if i%2 == 1:\n",
    "        class_name.append(name + '_unmentioned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Haoming/info_previously_extracted_from_ontologies/rxcui_ingredient.json', 'r') as fp:\n",
    "    rxn_relationships = json.load(fp)\n",
    "    \n",
    "rxn_relations = []\n",
    "rxn_entities = []\n",
    "\n",
    "for key, values in rxn_relationships.items():\n",
    "    for value in values:\n",
    "        if value not in rxn_entities:\n",
    "            rxn_entities.append(\"rxn_\" + value)\n",
    "        rxn_relations.append((\"rxn_\" + key, \"rxn_\" + value))\n",
    "        \n",
    "with open('../Haoming/info_previously_extracted_from_ontologies/snomed_parents_inferred.json', 'r') as fp:\n",
    "    sct_relationships = json.load(fp)\n",
    "\n",
    "sct_relations = []\n",
    "sct_entities = []\n",
    "\n",
    "for key, values in sct_relationships.items():\n",
    "    for value in values:\n",
    "        if value not in sct_entities:\n",
    "            sct_entities.append(\"sct_\" + value)\n",
    "        sct_relations.append((\"sct_\" + key, \"sct_\" + value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5379590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = []\n",
    "for name in entities:\n",
    "    absent = train_feature.loc[train_target[train_target[name] == -1].index.tolist()]\n",
    "    questionable = train_feature.loc[train_target[train_target[name] == 0].index.tolist()]\n",
    "    present = train_feature.loc[train_target[train_target[name] == -1].index.tolist()]        \n",
    "    unmentioned = train_feature.loc[train_target[train_target[name] == 3].index.tolist()]\n",
    "    \n",
    "    for value in features:\n",
    "        if absent[value].sum() != 0:\n",
    "            parents.append((name + '_absent', value))\n",
    "        if questionable[value].sum() != 0:\n",
    "            parents.append((name + '_questionable', value))\n",
    "        if present[value].sum() != 0:\n",
    "            parents.append((name + '_present', value))\n",
    "        if unmentioned[value].sum() != 0:\n",
    "            parents.append((name + '_unmentioned', value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fd9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = parents + rxn_relations + sct_relations\n",
    "entities = class_name + rxn_entities + sct_entities\n",
    "\n",
    "entities = np.unique(entities)\n",
    "parents = np.unique(parents, axis=0)\n",
    "\n",
    "all_relationships = list(itertools.product(entities, repeat=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239edea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import logging\n",
    "import math\n",
    "from numba import jit, cuda\n",
    "\n",
    "threads = []\n",
    "thread_num = 4\n",
    "array_len = math.floor(len(all_relationships) / thread_num)\n",
    "not_parents = []\n",
    "\n",
    "\n",
    "def helper_fun(sub_array):\n",
    "    result = [item for item in sub_array if item not in parents]\n",
    "    print(result[0])\n",
    "    np.concatenate((not_parents, result))\n",
    "\n",
    "@cuda.jit\n",
    "def setNoParents():\n",
    "    for i in range(thread_num):\n",
    "        start_index = array_len  * i\n",
    "        end_index = array_len * (i + 1)\n",
    "        sub_array = all_relationships[start_index : end_index]\n",
    "        print(\"Main    : create and start thread %d.\", i)\n",
    "        thread = threading.Thread(target=helper_fun, args=(sub_array,))\n",
    "        threads.append(thread)\n",
    "        thread.start()  \n",
    "        \n",
    "    for index, thread in enumerate(threads):\n",
    "        print(\"Main    : before joining thread %d.\", index)\n",
    "        thread.join()\n",
    "        print(\"Main    : thread %d done\", index)\n",
    "    \n",
    "threadsperblock = 32\n",
    "blockspergrid = (len(all_relationships) + (threadsperblock - 1)) // threadsperblock\n",
    "setNoParents[blockspergrid, threadsperblock]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1\n",
    "\n",
    "Parent = ltn.Predicate.MLP([embedding_size, embedding_size], hidden_layer_sizes=[8,8])\n",
    "Ancestor = ltn.Predicate.MLP([embedding_size, embedding_size], hidden_layer_sizes=[8,8])\n",
    "\n",
    "g_e = {\n",
    "    l: ltn.Constant(np.random.uniform(low=0, high=1, size=embedding_size), trainable=True)\n",
    "    for l in entites\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(p=5),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(p=5),semantics=\"exists\")\n",
    "\n",
    "formula_aggregator = ltn.Wrapper_Formula_Aggregator(ltn.fuzzy_ops.Aggreg_pMeanError(p=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6644333",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def axioms():\n",
    "    a = ltn.Variable.from_constants(\"a\", list(g_e.values()))\n",
    "    b = ltn.Variable.from_constants(\"b\", list(g_e.values()))\n",
    "    c = ltn.Variable.from_constants(\"c\", list(g_e.values()))\n",
    "\n",
    "\n",
    "    axioms = [\n",
    "        Forall((a,b), Implies(Parent([a,b]),Ancestor([a,b]))),\n",
    "        Forall(a, Not(Parent([a,a]))),\n",
    "        Forall(a, Not(Ancestor([a,a]))),\n",
    "        Forall((a,b), Implies(Parent([a,b]),Not(Parent([b,a])))),\n",
    "        Forall(\n",
    "            (a,b,c),\n",
    "            Implies(And(Parent([a,b]),Ancestor([b,c])), Ancestor([a,c])),\n",
    "            p=6\n",
    "        ),\n",
    "        Forall(\n",
    "            (a,b),\n",
    "            Implies(Ancestor([a,b]), \n",
    "                    Or(Parent([a,b]), \n",
    "                       Exists(c, And(Ancestor([a,c]),Parent([c,b])),p=6)\n",
    "                      )\n",
    "                   )\n",
    "        )\n",
    "    ]  \n",
    "    \n",
    "    sat_level = formula_aggregator(axioms).tensor\n",
    "    return sat_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc385d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial sat level %.5f\"%axioms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cacb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_variables = \\\n",
    "        Parent.trainable_variables\\\n",
    "        +Ancestor.trainable_variables \\\n",
    "        +ltn.as_tensors(list(g_e.values()))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = 1. - axioms()\n",
    "    grads = tape.gradient(loss_value, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "    if epoch%200 == 0:\n",
    "        print(\"Epoch %d: Sat Level %.3f\"%(epoch, axioms()))\n",
    "print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, axioms()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
